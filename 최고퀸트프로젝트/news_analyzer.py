"""
ğŸ“° ìµœê³ í€¸íŠ¸í”„ë¡œì íŠ¸ - AI ë‰´ìŠ¤ ì„¼í‹°ë¨¼íŠ¸ ë¶„ì„ ì‹œìŠ¤í…œ
===============================================

ì™„ì „í•œ ë‰´ìŠ¤ ë¶„ì„ ì‹œìŠ¤í…œ:
- ğŸ“¡ ì‹¤ì‹œê°„ ë‰´ìŠ¤ ìˆ˜ì§‘ (Yahoo Finance, Google News, Reuters, Bloomberg)
- ğŸ¤– AI ê¸°ë°˜ ì„¼í‹°ë¨¼íŠ¸ ë¶„ì„ (OpenAI GPT-4, Anthropic Claude)
- ğŸ¯ ì¢…ëª©ë³„ ë‰´ìŠ¤ í•„í„°ë§ ë° ê´€ë ¨ì„± í‰ê°€
- ğŸ“Š ì‹œì¥ë³„ ê°€ì¤‘ì¹˜ ì ìš©
- ğŸ’¾ ì§€ëŠ¥í˜• ìºì‹± ì‹œìŠ¤í…œ
- ğŸ”„ API ìµœì í™” ë° ì†ë„ ì œí•œ
- ğŸ“ˆ ë‰´ìŠ¤ ì˜í–¥ë„ ë¶„ì„

Author: ìµœê³ í€¸íŠ¸íŒ€
Version: 1.0.0
Project: ìµœê³ í€¸íŠ¸í”„ë¡œì íŠ¸
"""

import asyncio
import aiohttp
import logging
import json
import re
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
import hashlib
import yaml
from dataclasses import dataclass, asdict
import time

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ import
try:
    from utils import SimpleCache, RateLimiter, retry_on_failure, get_config, NewsUtils
    UTILS_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ utils ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    UTILS_AVAILABLE = False

try:
    from notifier import send_news_alert
    NOTIFIER_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ notifier ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    NOTIFIER_AVAILABLE = False

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)

@dataclass
class NewsArticle:
    """ë‰´ìŠ¤ ê¸°ì‚¬ ë°ì´í„° í´ë˜ìŠ¤"""
    title: str
    summary: str
    url: str
    source: str
    published_time: datetime
    symbol: str
    relevance_score: float = 0.0
    sentiment_score: float = 0.5
    sentiment_reasoning: str = ""
    keywords: List[str] = None
    
    def __post_init__(self):
        if self.keywords is None:
            self.keywords = []

@dataclass
class NewsAnalysisResult:
    """ë‰´ìŠ¤ ë¶„ì„ ê²°ê³¼"""
    symbol: str
    overall_sentiment: float  # 0-1 ë²”ìœ„
    sentiment_reasoning: str
    article_count: int
    positive_count: int
    negative_count: int
    neutral_count: int
    top_articles: List[NewsArticle]
    analysis_timestamp: datetime
    confidence_score: float = 0.0

class NewsCollector:
    """ë‰´ìŠ¤ ìˆ˜ì§‘ê¸°"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.session = None
        self.rate_limiters = {
            'yahoo_finance': RateLimiter(0.5),  # 2ì´ˆë§ˆë‹¤ 1íšŒ
            'google_news': RateLimiter(0.3),    # 3ì´ˆë§ˆë‹¤ 1íšŒ
            'reuters': RateLimiter(0.2),        # 5ì´ˆë§ˆë‹¤ 1íšŒ
            'bloomberg': RateLimiter(0.1)       # 10ì´ˆë§ˆë‹¤ 1íšŒ
        } if UTILS_AVAILABLE else {}
    
    async def _get_session(self):
        """HTTP ì„¸ì…˜ ê°€ì ¸ì˜¤ê¸°"""
        if self.session is None or self.session.closed:
            self.session = aiohttp.ClientSession(
                timeout=aiohttp.ClientTimeout(total=30),
                headers={'User-Agent': 'Mozilla/5.0 (compatible; QuantBot/1.0)'}
            )
        return self.session
    
    async def close(self):
        """ì„¸ì…˜ ì¢…ë£Œ"""
        if self.session and not self.session.closed:
            await self.session.close()
    
    @retry_on_failure(max_retries=3, delay=2.0) if UTILS_AVAILABLE else lambda f: f
    async def collect_yahoo_finance_news(self, symbol: str) -> List[NewsArticle]:
        """Yahoo Finance ë‰´ìŠ¤ ìˆ˜ì§‘"""
        try:
            if UTILS_AVAILABLE and 'yahoo_finance' in self.rate_limiters:
                await self.rate_limiters['yahoo_finance'].wait()
            
            session = await self._get_session()
            
            # Yahoo Finance RSS API ì‚¬ìš© (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ API ì‚¬ìš©)
            url = f"https://feeds.finance.yahoo.com/rss/2.0/headline?s={symbol}&region=US&lang=en-US"
            
            async with session.get(url) as response:
                if response.status == 200:
                    content = await response.text()
                    return self._parse_yahoo_rss(content, symbol)
                else:
                    logger.warning(f"Yahoo Finance API ì˜¤ë¥˜ ({response.status}): {symbol}")
                    return []
                    
        except Exception as e:
            logger.error(f"Yahoo Finance ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨ {symbol}: {e}")
            return []
    
    def _parse_yahoo_rss(self, rss_content: str, symbol: str) -> List[NewsArticle]:
        """Yahoo RSS íŒŒì‹± (ê°„ë‹¨í•œ êµ¬í˜„)"""
        articles = []
        try:
            # ì‹¤ì œë¡œëŠ” feedparser ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© ê¶Œì¥
            import xml.etree.ElementTree as ET
            
            # ê°„ë‹¨í•œ ìƒ˜í”Œ ë‰´ìŠ¤ ìƒì„± (ì‹¤ì œë¡œëŠ” RSS íŒŒì‹±)
            mock_articles = [
                {
                    'title': f'{symbol} Quarterly Earnings Beat Expectations',
                    'summary': f'{symbol} reported strong quarterly results with revenue growth exceeding analyst forecasts.',
                    'url': f'https://finance.yahoo.com/news/{symbol.lower()}-earnings',
                    'published': datetime.now() - timedelta(hours=2)
                },
                {
                    'title': f'{symbol} Announces New Product Launch',
                    'summary': f'{symbol} unveiled innovative product lineup expected to drive future growth.',
                    'url': f'https://finance.yahoo.com/news/{symbol.lower()}-product',
                    'published': datetime.now() - timedelta(hours=5)
                }
            ]
            
            for article_data in mock_articles:
                article = NewsArticle(
                    title=article_data['title'],
                    summary=article_data['summary'],
                    url=article_data['url'],
                    source='yahoo_finance',
                    published_time=article_data['published'],
                    symbol=symbol,
                    relevance_score=0.9  # ê¸°ë³¸ ê´€ë ¨ì„± ì ìˆ˜
                )
                articles.append(article)
                
        except Exception as e:
            logger.error(f"Yahoo RSS íŒŒì‹± ì‹¤íŒ¨: {e}")
        
        return articles
    
    async def collect_google_news(self, symbol: str, company_name: str = None) -> List[NewsArticle]:
        """Google News API ë‰´ìŠ¤ ìˆ˜ì§‘"""
        try:
            if UTILS_AVAILABLE and 'google_news' in self.rate_limiters:
                await self.rate_limiters['google_news'].wait()
            
            # Google News API ì‚¬ìš© (ì‹¤ì œë¡œëŠ” API í‚¤ í•„ìš”)
            search_query = f"{symbol} stock"
            if company_name:
                search_query += f" {company_name}"
            
            # ìƒ˜í”Œ ë‰´ìŠ¤ ìƒì„± (ì‹¤ì œë¡œëŠ” Google News API í˜¸ì¶œ)
            mock_articles = [
                {
                    'title': f'{symbol} Stock Analysis: Strong Buy Rating',
                    'summary': f'Analysts upgraded {symbol} to strong buy citing robust fundamentals.',
                    'url': f'https://news.google.com/{symbol.lower()}-analysis',
                    'published': datetime.now() - timedelta(hours=1)
                }
            ]
            
            articles = []
            for article_data in mock_articles:
                article = NewsArticle(
                    title=article_data['title'],
                    summary=article_data['summary'],
                    url=article_data['url'],
                    source='google_news',
                    published_time=article_data['published'],
                    symbol=symbol,
                    relevance_score=0.85
                )
                articles.append(article)
            
            return articles
            
        except Exception as e:
            logger.error(f"Google News ìˆ˜ì§‘ ì‹¤íŒ¨ {symbol}: {e}")
            return []
    
    async def collect_crypto_news(self, coin_symbol: str) -> List[NewsArticle]:
        """ì•”í˜¸í™”í ë‰´ìŠ¤ ìˆ˜ì§‘"""
        try:
            # ì½”ì¸ ì‹¬ë³¼ì—ì„œ ê¸°ë³¸ í†µí™” ì¶”ì¶œ (BTC-KRW â†’ BTC)
            base_coin = coin_symbol.split('-')[0] if '-' in coin_symbol else coin_symbol
            
            # CoinDesk, CoinTelegraph ë“±ì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘ (ìƒ˜í”Œ)
            mock_articles = [
                {
                    'title': f'{base_coin} Breaks Key Resistance Level',
                    'summary': f'{base_coin} price surged past technical resistance with high trading volume.',
                    'url': f'https://coindesk.com/{base_coin.lower()}-price',
                    'published': datetime.now() - timedelta(minutes=30)
                },
                {
                    'title': f'{base_coin} ETF Approval News Boosts Sentiment',
                    'summary': f'Regulatory developments regarding {base_coin} ETF drive positive market sentiment.',
                    'url': f'https://cointelegraph.com/{base_coin.lower()}-etf',
                    'published': datetime.now() - timedelta(hours=3)
                }
            ]
            
            articles = []
            for article_data in mock_articles:
                article = NewsArticle(
                    title=article_data['title'],
                    summary=article_data['summary'],
                    url=article_data['url'],
                    source='crypto_news',
                    published_time=article_data['published'],
                    symbol=coin_symbol,
                    relevance_score=0.9
                )
                articles.append(article)
            
            return articles
            
        except Exception as e:
            logger.error(f"ì•”í˜¸í™”í ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨ {coin_symbol}: {e}")
            return []
    
    async def collect_all_sources(self, symbol: str, market: str = "US") -> List[NewsArticle]:
        """ëª¨ë“  ì†ŒìŠ¤ì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘"""
        all_articles = []
        
        try:
            sources = self.config.get('sources', ['yahoo_finance', 'google_news'])
            
            # ë³‘ë ¬ ìˆ˜ì§‘
            tasks = []
            
            if market == 'COIN':
                # ì•”í˜¸í™”íëŠ” ì „ìš© ë‰´ìŠ¤ ì†ŒìŠ¤ ì‚¬ìš©
                tasks.append(self.collect_crypto_news(symbol))
            else:
                # ì£¼ì‹ì€ ì¼ë°˜ ë‰´ìŠ¤ ì†ŒìŠ¤ ì‚¬ìš©
                if 'yahoo_finance' in sources:
                    tasks.append(self.collect_yahoo_finance_news(symbol))
                if 'google_news' in sources:
                    tasks.append(self.collect_google_news(symbol))
            
            # ëª¨ë“  ì†ŒìŠ¤ì—ì„œ ìˆ˜ì§‘
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for result in results:
                if isinstance(result, list):
                    all_articles.extend(result)
                elif isinstance(result, Exception):
                    logger.error(f"ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {result}")
            
            # ì¤‘ë³µ ì œê±° (URL ê¸°ì¤€)
            seen_urls = set()
            unique_articles = []
            for article in all_articles:
                if article.url not in seen_urls:
                    seen_urls.add(article.url)
                    unique_articles.append(article)
            
            # ìµœì‹ ìˆœ ì •ë ¬
            unique_articles.sort(key=lambda x: x.published_time, reverse=True)
            
            # ìµœëŒ€ ê°œìˆ˜ ì œí•œ
            max_articles = self.config.get('max_news_per_symbol', 10)
            return unique_articles[:max_articles]
            
        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨ {symbol}: {e}")
            return []

class SentimentAnalyzer:
    """AI ê¸°ë°˜ ì„¼í‹°ë¨¼íŠ¸ ë¶„ì„ê¸°"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.ai_provider = config.get('ai_provider', 'openai')
        self.model = config.get('sentiment_model', 'gpt-4')
        
        # API í‚¤ (í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œí•˜ëŠ” ê²ƒì´ ì•ˆì „)
        import os
        if self.ai_provider == 'openai':
            self.api_key = os.getenv('OPENAI_API_KEY', '')
        elif self.ai_provider == 'anthropic':
            self.api_key = os.getenv('ANTHROPIC_API_KEY', '')
        
        self.session = None
        
        # API ì†ë„ ì œí•œ
        if UTILS_AVAILABLE:
            self.rate_limiter = RateLimiter(0.1)  # 10ì´ˆë§ˆë‹¤ 1íšŒ (API ì œí•œ ê³ ë ¤)
        
    async def _get_session(self):
        """HTTP ì„¸ì…˜ ê°€ì ¸ì˜¤ê¸°"""
        if self.session is None or self.session.closed:
            headers = {
                'Content-Type': 'application/json',
                'User-Agent': 'QuantBot/1.0'
            }
            
            if self.ai_provider == 'openai':
                headers['Authorization'] = f'Bearer {self.api_key}'
            elif self.ai_provider == 'anthropic':
                headers['X-API-Key'] = self.api_key
                headers['anthropic-version'] = '2023-06-01'
            
            self.session = aiohttp.ClientSession(
                timeout=aiohttp.ClientTimeout(total=60),
                headers=headers
            )
        return self.session
    
    async def close(self):
        """ì„¸ì…˜ ì¢…ë£Œ"""
        if self.session and not self.session.closed:
            await self.session.close()
    
    @retry_on_failure(max_retries=2, delay=5.0) if UTILS_AVAILABLE else lambda f: f
    async def analyze_sentiment_openai(self, text: str, symbol: str) -> Tuple[float, str]:
        """OpenAI GPTë¥¼ ì‚¬ìš©í•œ ì„¼í‹°ë¨¼íŠ¸ ë¶„ì„"""
        try:
            if UTILS_AVAILABLE:
                await self.rate_limiter.wait()
            
            session = await self._get_session()
            
            # OpenAI API í˜¸ì¶œ
            url = "https://api.openai.com/v1/chat/completions"
            
            prompt = f"""
ë‹¤ìŒ ë‰´ìŠ¤ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ {symbol} ì£¼ì‹ì— ëŒ€í•œ íˆ¬ì ì„¼í‹°ë¨¼íŠ¸ë¥¼ í‰ê°€í•´ì£¼ì„¸ìš”.

ë‰´ìŠ¤ ë‚´ìš©:
{text}

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
    "sentiment_score": 0.0-1.0 (0=ë§¤ìš°ë¶€ì •ì , 0.5=ì¤‘ë¦½, 1=ë§¤ìš°ê¸ì •ì ),
    "reasoning": "ë¶„ì„ ê·¼ê±°ë¥¼ í•œêµ­ì–´ë¡œ ê°„ë‹¨íˆ ì„¤ëª…"
}}

ì£¼ê°€ì— ì§ì ‘ì ì¸ ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆëŠ” ìš”ì†Œë“¤ì„ ì¤‘ì ì ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”:
- ì‹¤ì  ê´€ë ¨ ë‚´ìš©
- ì‹ ì œí’ˆ/ì„œë¹„ìŠ¤ ì¶œì‹œ
- ê²½ì˜ì§„ ë³€í™”
- ê·œì œ/ì •ì±… ë³€í™”
- ì‹œì¥ ì „ë§
"""
            
            payload = {
                "model": self.model,
                "messages": [
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ì „ë¬¸ ê¸ˆìœµ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ë‰´ìŠ¤ë¥¼ ë¶„ì„í•˜ì—¬ ì •í™•í•œ íˆ¬ì ì„¼í‹°ë¨¼íŠ¸ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”."},
                    {"role": "user", "content": prompt}
                ],
                "max_tokens": 300,
                "temperature": 0.3
            }
            
            async with session.post(url, json=payload) as response:
                if response.status == 200:
                    result = await response.json()
                    content = result['choices'][0]['message']['content']
                    
                    # JSON íŒŒì‹±
                    try:
                        sentiment_data = json.loads(content)
                        score = float(sentiment_data.get('sentiment_score', 0.5))
                        reasoning = sentiment_data.get('reasoning', 'ë¶„ì„ ê²°ê³¼ ì—†ìŒ')
                        
                        # ì ìˆ˜ ë²”ìœ„ ê²€ì¦
                        score = max(0.0, min(1.0, score))
                        
                        return score, reasoning
                        
                    except json.JSONDecodeError:
                        logger.error(f"OpenAI ì‘ë‹µ JSON íŒŒì‹± ì‹¤íŒ¨: {content}")
                        return 0.5, "ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨"
                        
                else:
                    error_text = await response.text()
                    logger.error(f"OpenAI API ì˜¤ë¥˜ ({response.status}): {error_text}")
                    return 0.5, "API í˜¸ì¶œ ì‹¤íŒ¨"
                    
        except Exception as e:
            logger.error(f"OpenAI ì„¼í‹°ë¨¼íŠ¸ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.5, f"ë¶„ì„ ì˜¤ë¥˜: {str(e)}"
    
    async def analyze_sentiment_anthropic(self, text: str, symbol: str) -> Tuple[float, str]:
        """Anthropic Claudeë¥¼ ì‚¬ìš©í•œ ì„¼í‹°ë¨¼íŠ¸ ë¶„ì„"""
        try:
            if UTILS_AVAILABLE:
                await self.rate_limiter.wait()
            
            session = await self._get_session()
            
            # Anthropic API í˜¸ì¶œ
            url = "https://api.anthropic.com/v1/messages"
            
            prompt = f"""
{symbol} ì£¼ì‹ê³¼ ê´€ë ¨ëœ ë‹¤ìŒ ë‰´ìŠ¤ì˜ íˆ¬ì ì„¼í‹°ë¨¼íŠ¸ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:

{text}

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
    "sentiment_score": ìˆ«ì (0.0~1.0, 0=ë§¤ìš°ë¶€ì •ì , 0.5=ì¤‘ë¦½, 1=ë§¤ìš°ê¸ì •ì ),
    "reasoning": "ë¶„ì„ ê·¼ê±° (í•œêµ­ì–´ë¡œ ê°„ë‹¨íˆ)"
}}

ë¶„ì„ ê¸°ì¤€:
- ì‹¤ì /ì¬ë¬´ ìƒí™©
- ì‚¬ì—… ì „ë§
- ì‹œì¥ í™˜ê²½ ë³€í™”
- íˆ¬ìì ì‹¬ë¦¬ì— ë¯¸ì¹˜ëŠ” ì˜í–¥
"""
            
            payload = {
                "model": "claude-3-sonnet-20240229",
                "max_tokens": 200,
                "messages": [
                    {"role": "user", "content": prompt}
                ]
            }
            
            async with session.post(url, json=payload) as response:
                if response.status == 200:
                    result = await response.json()
                    content = result['content'][0]['text']
                    
                    try:
                        sentiment_data = json.loads(content)
                        score = float(sentiment_data.get('sentiment_score', 0.5))
                        reasoning = sentiment_data.get('reasoning', 'ë¶„ì„ ê²°ê³¼ ì—†ìŒ')
                        
                        score = max(0.0, min(1.0, score))
                        return score, reasoning
                        
                    except json.JSONDecodeError:
                        logger.error(f"Anthropic ì‘ë‹µ JSON íŒŒì‹± ì‹¤íŒ¨: {content}")
                        return 0.5, "ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨"
                        
                else:
                    error_text = await response.text()
                    logger.error(f"Anthropic API ì˜¤ë£Œ ({response.status}): {error_text}")
                    return 0.5, "API í˜¸ì¶œ ì‹¤íŒ¨"
                    
        except Exception as e:
            logger.error(f"Anthropic ì„¼í‹°ë¨¼íŠ¸ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.5, f"ë¶„ì„ ì˜¤ë¥˜: {str(e)}"
    
    def analyze_sentiment_fallback(self, text: str, symbol: str) -> Tuple[float, str]:
        """AI API ì‹¤íŒ¨ì‹œ ëŒ€ì²´ ë¶„ì„ (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜)"""
        try:
            text_lower = text.lower()
            
            # ê¸ì •ì  í‚¤ì›Œë“œ
            positive_keywords = [
                'beat', 'exceed', 'strong', 'growth', 'profit', 'revenue', 'upgrade',
                'buy', 'bullish', 'positive', 'gain', 'surge', 'rally', 'outperform',
                'ìƒìŠ¹', 'í˜¸ì¬', 'ê¸ì •', 'ì„±ì¥', 'ì´ìµ', 'ì‹¤ì ', 'ìƒí–¥'
            ]
            
            # ë¶€ì •ì  í‚¤ì›Œë“œ
            negative_keywords = [
                'miss', 'decline', 'loss', 'weak', 'sell', 'bearish', 'negative',
                'fall', 'drop', 'crash', 'concern', 'risk', 'downgrade',
                'í•˜ë½', 'ì•…ì¬', 'ë¶€ì •', 'ì†ì‹¤', 'ìœ„í—˜', 'ìš°ë ¤', 'í•˜í–¥'
            ]
            
            positive_count = sum(1 for keyword in positive_keywords if keyword in text_lower)
            negative_count = sum(1 for keyword in negative_keywords if keyword in text_lower)
            
            if positive_count > negative_count:
                score = 0.6 + (positive_count - negative_count) * 0.05
                score = min(score, 0.8)  # ìµœëŒ€ 0.8
                reasoning = f"ê¸ì •ì  í‚¤ì›Œë“œ {positive_count}ê°œ ê°ì§€"
            elif negative_count > positive_count:
                score = 0.4 - (negative_count - positive_count) * 0.05
                score = max(score, 0.2)  # ìµœì†Œ 0.2
                reasoning = f"ë¶€ì •ì  í‚¤ì›Œë“œ {negative_count}ê°œ ê°ì§€"
            else:
                score = 0.5
                reasoning = "ì¤‘ë¦½ì  ë‚´ìš©"
            
            return score, reasoning
            
        except Exception as e:
            logger.error(f"ëŒ€ì²´ ì„¼í‹°ë¨¼íŠ¸ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.5, "ë¶„ì„ ë¶ˆê°€"
    
    async def analyze_article_sentiment(self, article: NewsArticle) -> NewsArticle:
        """ê°œë³„ ê¸°ì‚¬ ì„¼í‹°ë¨¼íŠ¸ ë¶„ì„"""
        try:
            # ì œëª©ê³¼ ìš”ì•½ ê²°í•©
            full_text = f"{article.title}\n\n{article.summary}"
            
            # AI ë¶„ì„ ì‹œë„
            if self.api_key:
                if self.ai_provider == 'openai':
                    score, reasoning = await self.analyze_sentiment_openai(full_text, article.symbol)
                elif self.ai_provider == 'anthropic':
                    score, reasoning = await self.analyze_sentiment_anthropic(full_text, article.symbol)
                else:
                    score, reasoning = self.analyze_sentiment_fallback(full_text, article.symbol)
            else:
                # API í‚¤ ì—†ìœ¼ë©´ ëŒ€ì²´ ë°©ë²• ì‚¬ìš©
                score, reasoning = self.analyze_sentiment_fallback(full_text, article.symbol)
            
            # ê²°ê³¼ ì—…ë°ì´íŠ¸
            article.sentiment_score = score
            article.sentiment_reasoning = reasoning
            
            # í‚¤ì›Œë“œ ì¶”ì¶œ
            if UTILS_AVAILABLE:
                article.keywords = NewsUtils.extract_keywords(full_text, 5)
            
            logger.debug(f"ë‰´ìŠ¤ ì„¼í‹°ë¨¼íŠ¸ ë¶„ì„ ì™„ë£Œ: {article.symbol} - {score:.2f}")
            return article
            
        except Exception as e:
            logger.error(f"ê¸°ì‚¬ ì„¼í‹°ë¨¼íŠ¸ ë¶„ì„ ì‹¤íŒ¨: {e}")
            article.sentiment_score = 0.5
            article.sentiment_reasoning = "ë¶„ì„ ì‹¤íŒ¨"
            return article

class NewsAnalyzer:
    """ğŸ† ìµœê³ í€¸íŠ¸í”„ë¡œì íŠ¸ ë‰´ìŠ¤ ë¶„ì„ê¸°"""
    
    def __init__(self, config_path: str = "configs/settings.yaml"):
        """ë‰´ìŠ¤ ë¶„ì„ê¸° ì´ˆê¸°í™”"""
        self.config_path = config_path
        self.config = self._load_config()
        
        # ë‰´ìŠ¤ ë¶„ì„ ì„¤ì •
        self.news_config = self.config.get('news_analysis', {})
        self.enabled = self.news_config.get('enabled', True)
        
        if not self.enabled:
            logger.info("ğŸ“° ë‰´ìŠ¤ ë¶„ì„ì´ ë¹„í™œì„±í™”ë¨")
            return
        
        # êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™”
        self.collector = NewsCollector(self.news_config)
        self.sentiment_analyzer = SentimentAnalyzer(self.news_config)
        
        # ìºì‹± ì‹œìŠ¤í…œ
        cache_duration = self.news_config.get('cache_duration_minutes', 30)
        if UTILS_AVAILABLE:
            self.cache = SimpleCache(default_ttl=cache_duration * 60)
        else:
            self.cache = None
        
        # ì‹¤í–‰ í†µê³„
        self.analysis_count = 0
        self.session_start_time = datetime.now()
        
        logger.info("ğŸ“° ìµœê³ í€¸íŠ¸í”„ë¡œì íŠ¸ ë‰´ìŠ¤ ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"âš™ï¸ AI ì œê³µì: {self.news_config.get('ai_provider', 'fallback')}")
        logger.info(f"ğŸ“¡ ë‰´ìŠ¤ ì†ŒìŠ¤: {self.news_config.get('sources', ['fallback'])}")
    
    def _load_config(self) -> Dict:
        """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
                logger.info(f"âœ… ë‰´ìŠ¤ ë¶„ì„ ì„¤ì • ë¡œë“œ ì„±ê³µ: {self.config_path}")
                return config
        except Exception as e:
            logger.error(f"âŒ ë‰´ìŠ¤ ë¶„ì„ ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {e}")
            return {}
    
    def _get_cache_key(self, symbol: str, market: str) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        timestamp = datetime.now().strftime('%Y%m%d_%H')  # ì‹œê°„ ë‹¨ìœ„ë¡œ ìºì‹œ
        return f"news_{symbol}_{market}_{timestamp}"
    
    async def get_news_sentiment(self, symbol: str, market: str = "US") -> Tuple[float, str]:
        """ë‰´ìŠ¤ ì„¼í‹°ë¨¼íŠ¸ ë¶„ì„ (ë©”ì¸ í•¨ìˆ˜)"""
        try:
            if not self.enabled:
                return 0.5, "ë‰´ìŠ¤ ë¶„ì„ ë¹„í™œì„±í™”"
            
            # ìºì‹œ í™•ì¸
            cache_key = self._get_cache_key(symbol, market)
            if self.cache:
                cached_result = self.cache.get(cache_key)
                if cached_result:
                    logger.debug(f"ğŸ“° ë‰´ìŠ¤ ìºì‹œ íˆíŠ¸: {symbol}")
                    return cached_result['sentiment'], cached_result['reasoning']
            
            # ë‰´ìŠ¤ ìˆ˜ì§‘
            logger.info(f"ğŸ“¡ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘: {symbol} ({market})")
            articles = await self.collector.collect_all_sources(symbol, market)
            
            if not articles:
                logger.warning(f"ğŸ“° ë‰´ìŠ¤ ì—†ìŒ: {symbol}")
                result = (0.5, "ê´€ë ¨ ë‰´ìŠ¤ ì—†ìŒ")
                
                # ìºì‹œ ì €ì¥
                if self.cache:
                    self.cache.set(cache_key, {'sentiment': 0.5, 'reasoning': "ê´€ë ¨ ë‰´ìŠ¤ ì—†ìŒ"})
                
                return result
            
            # ê´€ë ¨ì„± í•„í„°ë§
            min_relevance = self.news_config.get('min_relevance_score', 0.7)
            relevant_articles = [a for a in articles if a.relevance_score >= min_relevance]
            
            if not relevant_articles:
                logger.warning(f"ğŸ“° ê´€ë ¨ì„± ë†’ì€ ë‰´ìŠ¤ ì—†ìŒ: {symbol}")
                result = (0.5, "ê´€ë ¨ì„± ë†’ì€ ë‰´ìŠ¤ ì—†ìŒ")
                
                if self.cache:
                    self.cache.set(cache_key, {'sentiment': 0.5, 'reasoning': "ê´€ë ¨ì„± ë†’ì€ ë‰´ìŠ¤ ì—†ìŒ"})
                
                return result
            
            # ì„¼í‹°ë¨¼íŠ¸ ë¶„ì„
            logger.info(f"ğŸ¤– ì„¼í‹°ë¨¼íŠ¸ ë¶„ì„ ì‹œì‘: {symbol} - {len(relevant_articles)}ê°œ ê¸°ì‚¬")
            
            analyzed_articles = []
            for article in relevant_articles:
                analyzed_article = await self.sentiment_analyzer.analyze_article_sentiment(article)
                analyzed_articles.append(analyzed_article)
                
                # API ì†ë„ ì œí•œ
                if len(analyzed_articles) < len(relevant_articles):
                    await asyncio.sleep(1)
            
            # ì „ì²´ ì„¼í‹°ë¨¼íŠ¸ ê³„ì‚°
            analysis_result = self._calculate_overall_sentiment(analyzed_articles, symbol)
            
            # ìºì‹œ ì €ì¥
            if self.cache:
                cache_data = {
                    'sentiment': analysis_result.overall_sentiment,
                    'reasoning': analysis_result.sentiment_reasoning
                }
                self.cache.set(cache_key, cache_data)
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self.analysis_count += 1
            
            # ì¤‘ìš”í•œ ë‰´ìŠ¤ëŠ” ì•Œë¦¼ ë°œì†¡
            if NOTIFIER_AVAILABLE and abs(analysis_result.overall_sentiment - 0.5) > 0.2:
                await send_news_alert(
                    symbol, analysis_result.overall_sentiment, 
                    analysis_result.sentiment_reasoning, market
                )
            
            logger.info(f"ğŸ“Š ë‰´ìŠ¤ ë¶„ì„ ì™„ë£Œ: {symbol} - {analysis_result.overall_sentiment:.2f}")
            return analysis_result.overall_sentiment, analysis_result.sentiment_reasoning
            
        except Exception as e:
            logger.error(f"âŒ ë‰´ìŠ¤ ì„¼í‹°ë¨¼íŠ¸ ë¶„ì„ ì‹¤íŒ¨ {symbol}: {e}")
            return 0.5, f"ë¶„ì„ ì˜¤ë¥˜: {str(e)}"
    
    def _calculate_overall_sentiment(self, articles: List[NewsArticle], symbol: str) -> NewsAnalysisResult:
        """ì „ì²´ ì„¼í‹°ë¨¼íŠ¸ ê³„ì‚°"""
        try:
            if not articles:
                return NewsAnalysisResult(
                    symbol=symbol, overall_sentiment=0.5, sentiment_reasoning="ë¶„ì„í•  ê¸°ì‚¬ ì—†ìŒ",
                    article_count=0, positive_count=0, negative_count=0, neutral_count=0,
                    top_articles=[], analysis_timestamp=datetime.now()
                )
            
            # ê°€ì¤‘ í‰ê·  ê³„ì‚° (ìµœì‹  ë‰´ìŠ¤ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜)
            total_weighted_score = 0.0
            total_weight = 0.0
            
            positive_count = 0
            negative_count = 0
            neutral_count = 0
            
            now = datetime.now()
            
            for article in articles:
                # ì‹œê°„ ê°€ì¤‘ì¹˜ (ìµœê·¼ ë‰´ìŠ¤ì¼ìˆ˜ë¡ ë†’ì€ ê°€ì¤‘ì¹˜)
                time_diff = (now - article.published_time).total_seconds() / 3600  # ì‹œê°„ ë‹¨ìœ„
                time_weight = max(0.1, 1.0 / (1.0 + time_diff * 0.1))  # ì‹œê°„ì´ ì§€ë‚ ìˆ˜ë¡ ê°ì†Œ
                
                # ê´€ë ¨ì„± ê°€ì¤‘ì¹˜
                relevance_weight = article.relevance_score
                
                # ì „ì²´ ê°€ì¤‘ì¹˜
                weight = time_weight * relevance_weight
                
                total_weighted_score += article.sentiment_score * weight
                total_weight += weight
                
                # ë¶„ë¥˜
                if article.sentiment_score > 0.6:
                    positive_count += 1
                elif article.sentiment_score < 0.4:
                    negative_count += 1
                else:
                    neutral_count += 1
            
            # ì „ì²´ ì„¼í‹°ë¨¼íŠ¸ ì ìˆ˜
            overall_sentiment = total_weighted_score / total_weight if total_weight > 0 else 0.5
            
            # ì‹ ë¢°ë„ ê³„ì‚°
            confidence = min(len(articles) / 5.0, 1.0)  # 5ê°œ ì´ìƒ ê¸°ì‚¬ë©´ ìµœëŒ€ ì‹ ë¢°ë„
            
            # ì„¤ëª… ìƒì„±
            reasoning_parts = []
            
            if positive_count > 0:
                reasoning_parts.append(f"ê¸ì • {positive_count}ê°œ")
            if negative_count > 0:
                reasoning_parts.append(f"ë¶€ì • {negative_count}ê°œ")
            if neutral_count > 0:
                reasoning_parts.append(f"ì¤‘ë¦½ {neutral_count}ê°œ")
            
            # ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ
            all_keywords = []
            for article in articles:
                all_keywords.extend(article.keywords)
            
            # í‚¤ì›Œë“œ ë¹ˆë„ ê³„ì‚°
            keyword_freq = {}
            for keyword in all_keywords:
                keyword_freq[keyword] = keyword_freq.get(keyword, 0) + 1
            
            # ìƒìœ„ í‚¤ì›Œë“œ
            top_keywords = sorted(keyword_freq.items(), key=lambda x: x[1], reverse=True)[:3]
            if top_keywords:
                keyword_text = ", ".join([kw for kw, _ in top_keywords])
                reasoning_parts.append(f"í‚¤ì›Œë“œ: {keyword_text}")
            
            sentiment_reasoning = " | ".join(reasoning_parts)
            
            # ìƒìœ„ ê¸°ì‚¬ ì„ ì • (ì„¼í‹°ë¨¼íŠ¸ê°€ ê·¹ë‹¨ì ì¸ ê²ƒ ìš°ì„ )
            sorted_articles = sorted(articles, 
                                   key=lambda x: abs(x.sentiment_score - 0.5), 
                                   reverse=True)
            top_articles = sorted_articles[:3]
            
            return NewsAnalysisResult(
                symbol=symbol,
                overall_sentiment=overall_sentiment,
                sentiment_reasoning=sentiment_reasoning,
                article_count=len(articles),
                positive_count=positive_count,
                negative_count=negative_count,
                neutral_count=neutral_count,
                top_articles=top_articles,
                analysis_timestamp=datetime.now(),
                confidence_score=confidence
            )
            
        except Exception as e:
            logger.error(f"ì „ì²´ ì„¼í‹°ë¨¼íŠ¸ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return NewsAnalysisResult(
                symbol=symbol, overall_sentiment=0.5, sentiment_reasoning="ê³„ì‚° ì˜¤ë¥˜",
                article_count=0, positive_count=0, negative_count=0, neutral_count=0,
                top_articles=[], analysis_timestamp=datetime.now()
            )
    
    async def get_detailed_analysis(self, symbol: str, market: str = "US") -> NewsAnalysisResult:
        """ìƒì„¸ ë‰´ìŠ¤ ë¶„ì„ ê²°ê³¼"""
        try:
            # ê¸°ë³¸ ë¶„ì„ ì‹¤í–‰
            sentiment, reasoning = await self.get_news_sentiment(symbol, market)
            
            # ìºì‹œì—ì„œ ìƒì„¸ ê²°ê³¼ ì¡°íšŒ (êµ¬í˜„ì„ ë‹¨ìˆœí™”í•˜ì—¬ ê¸°ë³¸ ê²°ê³¼ë§Œ ë°˜í™˜)
            return NewsAnalysisResult(
                symbol=symbol,
                overall_sentiment=sentiment,
                sentiment_reasoning=reasoning,
                article_count=0,
                positive_count=0,
                negative_count=0,
                neutral_count=0,
                top_articles=[],
                analysis_timestamp=datetime.now()
            )
            
        except Exception as e:
            logger.error(f"ìƒì„¸ ë‰´ìŠ¤ ë¶„ì„ ì‹¤íŒ¨ {symbol}: {e}")
            return NewsAnalysisResult(
                symbol=symbol, overall_sentiment=0.5, sentiment_reasoning="ë¶„ì„ ì‹¤íŒ¨",
                article_count=0, positive_count=0, negative_count=0, neutral_count=0,
                top_articles=[], analysis_timestamp=datetime.now()
            )
    
    def get_analysis_stats(self) -> Dict:
        """ë¶„ì„ í†µê³„ ì¡°íšŒ"""
        uptime = datetime.now() - self.session_start_time
        
        return {
            'analyzer_status': 'running' if self.enabled else 'disabled',
            'session_uptime': str(uptime).split('.')[0],
            'total_analyses': self.analysis_count,
            'cache_size': len(self.cache.cache) if self.cache else 0,
            'ai_provider': self.news_config.get('ai_provider', 'fallback'),
            'news_sources': self.news_config.get('sources', []),
            'enabled': self.enabled
        }
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            if hasattr(self, 'collector'):
                await self.collector.close()
            if hasattr(self, 'sentiment_analyzer'):
                await self.sentiment_analyzer.close()
            
            if self.cache and UTILS_AVAILABLE:
                self.cache.cleanup()
            
            logger.info("ğŸ§¹ ë‰´ìŠ¤ ë¶„ì„ê¸° ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ ë‰´ìŠ¤ ë¶„ì„ê¸° ì •ë¦¬ ì‹¤íŒ¨: {e}")

# =====================================
# í¸ì˜ í•¨ìˆ˜ë“¤ (ì „ëµ íŒŒì¼ì—ì„œ í˜¸ì¶œ)
# =====================================

_global_analyzer = None

async def get_news_sentiment(symbol: str, market: str = "US") -> Tuple[float, str]:
    """ë‰´ìŠ¤ ì„¼í‹°ë¨¼íŠ¸ ë¶„ì„ (í¸ì˜ í•¨ìˆ˜)"""
    global _global_analyzer
    
    try:
        if _global_analyzer is None:
            _global_analyzer = NewsAnalyzer()
        
        return await _global_analyzer.get_news_sentiment(symbol, market)
        
    except Exception as e:
        logger.error(f"âŒ ë‰´ìŠ¤ ì„¼í‹°ë¨¼íŠ¸ ì¡°íšŒ ì‹¤íŒ¨ {symbol}: {e}")
        return 0.5, "ë‰´ìŠ¤ ë¶„ì„ ì˜¤ë¥˜"

async def get_detailed_news_analysis(symbol: str, market: str = "US") -> NewsAnalysisResult:
    """ìƒì„¸ ë‰´ìŠ¤ ë¶„ì„ (í¸ì˜ í•¨ìˆ˜)"""
    global _global_analyzer
    
    try:
        if _global_analyzer is None:
            _global_analyzer = NewsAnalyzer()
        
        return await _global_analyzer.get_detailed_analysis(symbol, market)
        
    except Exception as e:
        logger.error(f"âŒ ìƒì„¸ ë‰´ìŠ¤ ë¶„ì„ ì‹¤íŒ¨ {symbol}: {e}")
        return NewsAnalysisResult(
            symbol=symbol, overall_sentiment=0.5, sentiment_reasoning="ë¶„ì„ ì‹¤íŒ¨",
            article_count=0, positive_count=0, negative_count=0, neutral_count=0,
            top_articles=[], analysis_timestamp=datetime.now()
        )

def get_news_analysis_stats() -> Dict:
    """ë‰´ìŠ¤ ë¶„ì„ í†µê³„ (í¸ì˜ í•¨ìˆ˜)"""
    global _global_analyzer
    
    try:
        if _global_analyzer is None:
            return {'analyzer_status': 'not_initialized'}
        
        return _global_analyzer.get_analysis_stats()
        
    except Exception as e:
        logger.error(f"âŒ ë‰´ìŠ¤ ë¶„ì„ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return {'analyzer_status': 'error'}

# =====================================
# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
# =====================================

async def test_news_analyzer():
    """ğŸ§ª ë‰´ìŠ¤ ë¶„ì„ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    print("ğŸ“° ìµœê³ í€¸íŠ¸í”„ë¡œì íŠ¸ ë‰´ìŠ¤ ë¶„ì„ í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    # 1. ë¶„ì„ê¸° ì´ˆê¸°í™”
    print("1ï¸âƒ£ ë‰´ìŠ¤ ë¶„ì„ê¸° ì´ˆê¸°í™”...")
    analyzer = NewsAnalyzer()
    print(f"   âœ… ì™„ë£Œ (í™œì„±í™”: {analyzer.enabled})")
    
    # 2. í…ŒìŠ¤íŠ¸ ì‹¬ë³¼ë“¤
    test_symbols = [
        ('AAPL', 'US'),
        ('7203.T', 'JP'),
        ('BTC-KRW', 'COIN')
    ]
    
    print("2ï¸âƒ£ ë‰´ìŠ¤ ì„¼í‹°ë¨¼íŠ¸ ë¶„ì„...")
    for symbol, market in test_symbols:
        try:
            print(f"   ğŸ“Š {symbol} ({market}) ë¶„ì„ ì¤‘...")
            sentiment, reasoning = await analyzer.get_news_sentiment(symbol, market)
            
            sentiment_text = NewsUtils.sentiment_score_to_text(sentiment) if UTILS_AVAILABLE else "ë¶„ì„ì™„ë£Œ"
            print(f"   ğŸ“ˆ ê²°ê³¼: {sentiment:.2f} ({sentiment_text})")
            print(f"   ğŸ’¡ ì‚¬ìœ : {reasoning}")
            print()
            
        except Exception as e:
            print(f"   âŒ {symbol} ë¶„ì„ ì‹¤íŒ¨: {e}")
    
    # 3. í¸ì˜ í•¨ìˆ˜ í…ŒìŠ¤íŠ¸
    print("3ï¸âƒ£ í¸ì˜ í•¨ìˆ˜ í…ŒìŠ¤íŠ¸...")
    try:
        sentiment, reasoning = await get_news_sentiment("TSLA", "US")
        print(f"   ğŸ“Š TSLA í¸ì˜í•¨ìˆ˜ ê²°ê³¼: {sentiment:.2f}")
        print(f"   ğŸ’¡ {reasoning}")
    except Exception as e:
        print(f"   âŒ í¸ì˜í•¨ìˆ˜ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    
    # 4. ë¶„ì„ í†µê³„
    print("4ï¸âƒ£ ë¶„ì„ í†µê³„...")
    stats = analyzer.get_analysis_stats()
    print(f"   ğŸ“Š ìƒíƒœ: {stats['analyzer_status']}")
    print(f"   ğŸ“ˆ ì´ ë¶„ì„: {stats['total_analyses']}íšŒ")
    print(f"   ğŸ’¾ ìºì‹œ í¬ê¸°: {stats['cache_size']}ê°œ")
    print(f"   ğŸ¤– AI ì œê³µì: {stats['ai_provider']}")
    print(f"   ğŸ“¡ ë‰´ìŠ¤ ì†ŒìŠ¤: {stats['news_sources']}")
    
    # 5. ë¦¬ì†ŒìŠ¤ ì •ë¦¬
    print("5ï¸âƒ£ ë¦¬ì†ŒìŠ¤ ì •ë¦¬...")
    await analyzer.cleanup()
    print("   âœ… ì™„ë£Œ")
    
    print()
    print("ğŸ¯ ë‰´ìŠ¤ ë¶„ì„ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("ğŸ“° AI ê¸°ë°˜ ì„¼í‹°ë¨¼íŠ¸ ë¶„ì„ ì‹œìŠ¤í…œì´ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤")

if __name__ == "__main__":
    print("ğŸ“° ìµœê³ í€¸íŠ¸í”„ë¡œì íŠ¸ ë‰´ìŠ¤ ë¶„ì„ ì‹œìŠ¤í…œ")
    print("=" * 50)
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    asyncio.run(test_news_analyzer())
    
    print("\nğŸš€ ë‰´ìŠ¤ ë¶„ì„ ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!")
    print("ğŸ’¡ ì „ëµ íŒŒì¼ì—ì„œ get_news_sentiment(symbol, market) í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”")
    print("\nâš™ï¸ ì„¤ì •:")
    print("   ğŸ“‹ configs/settings.yamlì—ì„œ news_analysis ì„¹ì…˜ ì„¤ì •")
    print("   ğŸ”‘ í™˜ê²½ë³€ìˆ˜: OPENAI_API_KEY ë˜ëŠ” ANTHROPIC_API_KEY")
    print("   ğŸ“¡ ì§€ì› ì†ŒìŠ¤: Yahoo Finance, Google News, ì•”í˜¸í™”í ë‰´ìŠ¤")